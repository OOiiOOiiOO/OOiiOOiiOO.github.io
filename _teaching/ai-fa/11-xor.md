---
title: "حل XOR با یک پرسپترون چندلایه (MLP)"
collection: teaching
permalink: /teaching/ai/xor
course: "هوش مصنوعی"
order: 1
mathjax: true
use_math: true
layout: course
---

# حل XOR با یک پرسپترون چندلایه (MLP)

تابع **XOR** (یا «یاگر انحصاری») زمانی مقدار 1 برمی‌گرداند که **دقیقاً یکی** از ورودی‌ها برابر 1 باشد، و در غیر این صورت مقدار 0.  
این تابع یک مثال کلاسیک است از مسئله‌ای که یک **پرسپترون تکی نمی‌تواند حل کند**، اما یک **MLP با یک لایه پنهان** قادر به حل آن است.

---

## 1) جدول درستی XOR

| $$x_1$$ | $$x_2$$ | XOR |
|:------:|:------:|:---:|
| 0      | 0      | 0   |
| 0      | 1      | 1   |
| 1      | 0      | 1   |
| 1      | 1      | 0   |

![XOR table](/images/ai/xor/xor_table.png)  
*شکل: جدول درستی XOR.*

---

## 2) چرا پرسپترون تکی شکست می‌خورد (غیرقابل جداسازی خطی)

یک پرسپترون یک مرز خطی ترسیم می‌کند:

$$
y = 
\begin{cases}
1 & \text{اگر } \mathbf{w}^\top \mathbf{x} + b \ge 0 \\
0 & \text{در غیر این صورت}
\end{cases}
$$

در XOR، نقاط مثبت $$(0,1)$$ و $$(1,0)$$ در دو گوشه مقابل قرار دارند، در حالی که نقاط منفی $$(0,0)$$ و $$(1,1)$$ در دو گوشه دیگر هستند.  
هیچ خط مستقیمی وجود ندارد که بتواند نقاط مثبت و منفی را از هم جدا کند.

![Not linearly separable](/images/ai/xor/not_linearly_separable.png)  
*شکل: داده‌های XOR با یک خط قابل جداسازی نیستند.*

---

## 3) ایده: شکستن XOR به گیت‌های ساده‌تر

یک هویت مفید:

$$
\text{XOR}(x_1,x_2) \;=\; \big(\,x_1 \lor x_2\,\big) \;\land\; \big(\,\lnot(x_1 \land x_2)\,\big).
$$

پس با **دو نورون در لایه پنهان** می‌توان محاسبه کرد:

- $$h_1 = \text{OR}(x_1,x_2)$$  
- $$h_2 = \text{NAND}(x_1,x_2)$$  

سپس **خروجی** برابر است با:

$$
y = \text{AND}(h_1, h_2).
$$

این دقیقاً یک **MLP با دو نورون پنهان** است (پنهان: $$h_1, h_2$$؛ خروجی: $$y$$).

![MLP architecture](/images/ai/xor/mlp_arch.png)  
*شکل: معماری MLP با دو نورون پنهان.*

---

## 4) انتخاب وزن‌ها در سبک پرسپترون (تابع پله‌ای)

می‌توان وزن‌ها و بایاس‌ها را به گونه‌ای تنظیم کرد که گیت‌های **OR**، **NAND** و **AND** را شبیه‌سازی کنند.

- **OR**: $$w_1=1,\; w_2=1,\; b=-0.5$$  
- **NAND**: $$w_1=-1,\; w_2=-1,\; b=1.5$$  
- **AND** (خروجی): $$v_1=1,\; v_2=1,\; b_{\text{out}}=-1.5$$  

لایه پنهان:

$$
h_1 = \mathbb{1}\!\big(1 \cdot x_1 + 1 \cdot x_2 - 0.5 \ge 0\big) = \text{OR}(x_1,x_2),
$$

$$
h_2 = \mathbb{1}\!\big(-1 \cdot x_1 - 1 \cdot x_2 + 1.5 \ge 0\big) = \text{NAND}(x_1,x_2).
$$

خروجی:

$$
y = \mathbb{1}\!\big(1 \cdot h_1 + 1 \cdot h_2 - 1.5 \ge 0\big) = \text{AND}(h_1,h_2).
$$

با بررسی هر چهار ورودی $$(0,0), (0,1), (1,0), (1,1)$$ مشاهده می‌کنیم که خروجی دقیقاً مطابق XOR است.

![Hidden regions](/images/ai/xor/hidden_regions.png)  
*شکل: نورون‌های پنهان فضای ورودی را به نواحی مختلف تقسیم می‌کنند.*

---

## 5) MLP نرم با تابع سیگموید (لجستیک)

برای مشتق‌پذیری (جهت گرادیان کاهشی)، به جای پله‌ای از **سیگموید** استفاده می‌کنیم:

$$
\sigma(z) = \frac{1}{1+e^{-z}}.
$$

یک مجموعه پارامتر مناسب برای تقریب XOR (ورودی‌ها $$x_1,x_2 \in \{0,1\}$$):

**لایه پنهان (۲ نورون):**

- $$h_1 \approx \text{OR}$$:  
  $$w^{(1)}_{1} = [20, 20], \quad b^{(1)}_{1} = -10$$  
- $$h_2 \approx \text{NAND}$$:  
  $$w^{(1)}_{2} = [-20, -20], \quad b^{(1)}_{2} = 30$$  

**لایه خروجی (AND روی $$h_1,h_2$$):**

- $$w^{(2)} = [20, 20], \quad b^{(2)} = -30$$  

گذر رو به جلو:

$$
z^{(1)}_k = \sum_{i=1}^2 w^{(1)}_{k,i} x_i + b^{(1)}_{k}, \qquad h_k = \sigma(z^{(1)}_k), \quad k \in \{1,2\}
$$

$$
z^{(2)} = \sum_{k=1}^2 w^{(2)}_{k} h_k + b^{(2)}, \qquad \hat{y} = \sigma(z^{(2)})
$$

**بررسی یک ورودی** $$(x_1,x_2) = (1,0)$$:

- $$z^{(1)}_1 = 20(1)+20(0)-10 = 10 \;\Rightarrow\; h_1 \approx \sigma(10) \approx 0.99995$$  
- $$z^{(1)}_2 = -20(1)-20(0)+30 = 10 \;\Rightarrow\; h_2 \approx 0.99995$$  
- $$z^{(2)} = 20(0.99995) + 20(0.99995) - 30 \approx 9.999 \;\Rightarrow\; \hat{y} \approx 0.99995 \approx 1$$  

به‌طور مشابه، برای ورودی‌های $$(0,1)$$ خروجی 1 و برای $$(0,0)$$ و $$(1,1)$$ خروجی 0 خواهد بود.  
(وزن‌های بزرگ باعث می‌شوند سیگموید رفتاری شبیه گیت منطقی داشته باشد.)

![Forward pass](/images/ai/xor/forward_pass.png)  
*شکل: محاسبات گذر رو به جلو در شبکه XOR.*

---

## 6) طرح کلی آموزش (یادگیری وزن‌ها)

چهار نمونه آموزشی:

$$
\mathcal{D} = \{([0,0],0), ([0,1],1), ([1,0],1), ([1,1],0)\}
$$

تابع هزینه **آنتروپی متقاطع** با خروجی سیگموید:

$$
\mathcal{L}(\theta) = -\frac{1}{4}\sum_{(x,y)\in\mathcal{D}} \Big[ y\log \hat{y} + (1-y)\log(1-\hat{y}) \Big]
$$

که در آن $$\hat{y} = \sigma(z^{(2)})$$ پیش‌بینی مدل است و $$\theta$$ شامل همه وزن‌ها و بایاس‌ها.  

سپس با **گرادیان کاهشی و پس‌انتشار خطا (Backpropagation)** آموزش داده می‌شود تا مدل همگرا شود.

---

## 7) نکات کلیدی

- XOR **خطی جداسازی‌پذیر نیست**، بنابراین یک پرسپترون تکی آن را حل نمی‌کند.  
- یک MLP با **یک لایه پنهان و دو نورون** می‌تواند XOR را حل کند.  
- با سیگموید و وزن‌های بزرگ می‌توان رفتار منطقی را تقریب زد؛ با آموزش نیز مدل می‌تواند این پارامترها را بیاموزد.  

---

## (اختیاری) محل قرار دادن تصاویر

- جدول درستی: `![XOR table](/images/ai/xor/xor_table.png)`  
- غیرقابل جداسازی خطی: `![Not linearly separable](/images/ai/xor/not_linearly_separable.png)`  
- معماری شبکه: `![MLP architecture](/images/ai/xor/mlp_arch.png)`  
- نواحی تصمیم نورون‌های پنهان: `![Hidden regions](/images/ai/xor/hidden_regions.png)`  
- گذر رو به جلو: `![Forward pass](/images/ai/xor/forward_pass.png)`  

---

## (اختیاری) دکمه‌های رفت و برگشت بین درس‌ها

<div class="lesson-nav">
  <a class="btn lesson-prev" href="{{ '/teaching/ai/logic-gates' | relative_url }}">◀︎ قبلی: پیاده‌سازی گیت‌های منطقی با پرسپترون</a>
  <a class="btn lesson-next" href="{{ '/teaching/ai/xor-notebook' | relative_url }}">بعدی: XOR – نوت‌بوک ▶︎</a>
</div>
