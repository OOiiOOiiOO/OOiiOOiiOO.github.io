---
title: "توابع فعال‌سازی در شبکه‌های عصبی"
collection: teaching
permalink: /teaching/ai/activation-functions
course: "هوش مصنوعی"
order: 1
mathjax: true
use_math: true
layout: course
---

# توابع فعال‌سازی

شبکه‌های عصبی مصنوعی (ANNs) قدرتمند هستند زیرا می‌توانند **توابع غیرخطی** را تقریب بزنند.  
اگر توابع فعال‌سازی وجود نداشته باشند، حتی اگر چندین لایه روی هم قرار دهیم، شبکه همچنان معادل یک **تبدیل خطی واحد** خواهد بود. این یعنی شبکه نمی‌تواند روابط پیچیده مثل الگوهای تصویری، سیگنال‌های صوتی یا مرزهای تصمیم‌گیری را مدل کند.

توابع فعال‌سازی با افزودن **غیرخطی بودن** به شبکه، امکان یادگیری نگاشت‌های بسیار پیچیده از ورودی به خروجی را فراهم می‌کنند. هر تابع ویژگی‌ها، مزایا و معایب خاص خود را دارد.

---

## ۱. تابع خطی (Linear)

**فرمول:**

$$
f(x) = x
$$

**مثال:**

$$
f(2.5) = 2.5
$$

**مزایا:**
- بسیار ساده، بدون مشکل گرادیان محو (Vanishing Gradient).  

**معایب:**
- توانایی مدل‌سازی داده‌های غیرخطی را ندارد.  
- برای لایه‌های پنهان عمقی بی‌فایده است.  

![General Formula](/images/ai5.avif)

---

## ۲. تابع سیگموید (Sigmoid)

**فرمول:**

$$
f(x) = \frac{1}{1 + e^{-x}}
$$

**مثال‌ها:**

$$
f(0) = 0.5
$$

$$
f(2) \approx 0.88
$$

$$
f(-3) \approx 0.047
$$

**مزایا:**
- منحنی صاف و قابل مشتق‌گیری در همه نقاط.  
- تفسیر احتمالاتی خوب.  

**معایب:**
- مشکل گرادیان محو برای مقادیر بزرگ $$x$$.  
- خروجی صفرمرکز (Zero-centered) نیست.  

![General Formula](/images/ai11.ppm)

---

## ۳. تانژانت هیپربولیک (Tanh)

**فرمول:**

$$
f(x) = \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
$$

**مثال‌ها:**

$$
f(0) = 0
$$

$$
f(1) \approx 0.76
$$

$$
f(-2) \approx -0.96
$$

**مزایا:**
- خروجی صفرمرکز → همگرایی سریع‌تر.  
- گرادیان قوی‌تر نسبت به سیگموید برای $$x$$های کوچک.  

**معایب:**
- همچنان مشکل گرادیان محو برای مقادیر خیلی بزرگ $$x$$ را دارد.  

![General Formula](/images/ai7.png)

---

## ۴. تابع ReLU

**فرمول:**

$$
f(x) = \max(0, x)
$$

**مثال‌ها:**

$$
f(-3) = 0
$$

$$
f(2.5) = 2.5
$$

**مزایا:**
- محاسباتی بسیار کارآمد.  
- کاهش مشکل گرادیان محو.  

**معایب:**
- مشکل «نورون مرده» (Dead Neuron).  

![General Formula](/images/ai8.png)

---

## ۵. Leaky ReLU

**فرمول:**

$$
f(x) =
\begin{cases}
x & \text{اگر } x \geq 0 \\
\alpha x & \text{اگر } x < 0
\end{cases}
$$

**مثال (با $$\alpha = 0.01$$):**

$$
f(-5) = -0.05, \quad f(3) = 3
$$

**مزایا:**
- جلوگیری از مشکل نورون مرده.  
- حفظ مزایای ReLU.  

**معایب:**
- کمی پیچیده‌تر از ReLU.  

![General Formula](/images/ai9.png)

---

## ۶. تابع Softmax

**فرمول:**

$$
f(x_i) = \frac{e^{x_i}}{\sum_{j=1}^{n} e^{x_j}}
$$

**مثال:**

ورودی: $[2.0, 1.0, 0.1]$

$$
f(2.0) = \frac{e^2}{e^2 + e^1 + e^{0.1}} \approx 0.71
$$

$$
f(1.0) \approx 0.26, \quad f(0.1) \approx 0.03
$$

**مزایا:**
- نرمال‌سازی خروجی به احتمال‌ها.  
- ضروری برای دسته‌بندی چندکلاسه.  

**معایب:**
- محاسبات سنگین‌تر.  
- حساسیت به مقادیر ورودی بزرگ.  

![General Formula](/images/ai10.png)

---

## 📊 جدول مقایسه

| تابع        | فرمول | بازه | مزایا | معایب |
|-------------|-------|------|-------|-------|
| خطی (Linear)     | \(f(x) = x\) | \((-\infty, \infty)\) | سادگی، مناسب رگرسیون | بدون غیرخطی بودن |
| سیگموید (Sigmoid) | \(f(x) = \frac{1}{1+e^{-x}}\) | \([0,1]\) | احتمال‌ها | مشکل گرادیان محو |
| تانژانت (Tanh)    | \(f(x) = \tanh(x)\) | \([-1,1]\) | صفرمرکز | گرادیان محو |
| ReLU        | \(f(x) = \max(0,x)\) | \([0,\infty)\) | سریع، پرکاربرد | نورون مرده |
| Softmax     | \(f(z_i) = \frac{e^{z_i}}{\sum_j e^{z_j}}\) | \([0,1]\)، جمع=۱ | احتمال‌های چندکلاسه | سنگین محاسباتی |

---

# مقایسه نهایی

| تابع        | بازه | مزایا | معایب |
|-------------|------|-------|-------|
| خطی         | $(-\infty,\infty)$ | ساده | بدون غیرخطی بودن |
| سیگموید     | $(0,1)$ | تفسیر احتمالاتی | گرادیان محو، صفرمرکز نیست |
| تانژانت     | $(-1,1)$ | صفرمرکز، همگرایی بهتر | گرادیان محو |
| ReLU        | $[0,\infty)$ | سریع و محبوب | نورون مرده |
| Leaky ReLU  | $(-\infty,\infty)$ | رفع مشکل نورون مرده | کمی پیچیده‌تر |
| Softmax     | $(0,1)$ | احتمال‌ها، جمع=۱ | پرهزینه، ناپایدار برای مقادیر بزرگ |

---

## ✅ جمع‌بندی

- توابع فعال‌سازی **غیرخطی بودن** را وارد شبکه می‌کنند.  
- انتخاب تابع بستگی به **نوع کار** (طبقه‌بندی، رگرسیون و غیره) دارد.  
- **ReLU** پیش‌فرض در یادگیری عمیق است.  
- **Softmax** برای خروجی‌های چندکلاسه استفاده می‌شود.  

---

<div class="lesson-nav" style="display:flex; justify-content:space-between; margin-top:2em;">
  <a class="btn btn--inverse" href="{{ '/teaching/ai/mathmaticalnl' | relative_url }}">⬅︎ قبلی: مدل ریاضی نورون </a>
  <a class="btn btn--primary" href="{{ '/teaching/ai/multi-class' | relative_url }}">بعدی: طبقه‌بندی چندکلاسه ➡︎</a>
</div>
