---
title: "Homework 2"
collection: teaching
permalink: /teaching/ai/hw2
course: "Artificial Intelligence"
order: 1
mathjax: true
use_math: true
layout: course
lang: en
alt_lang: fa
alt_url: /teaching/ai-fa/hw2
---



# Homework 2 – Activation Functions (with Solutions)

---

## Q1. Conceptual (2 pts)

**Q:**  
- Compare the **Sigmoid** and **ReLU** activation functions.  
- What is one main drawback of each?  
---

## Q2. Numerical (3 pts)

**Q:**  
Given input $z = -2$, compute the output for:  
1. Sigmoid  
2. Tanh  
3. ReLU  

---

## Q3. Applied (5 pts)

**Q:**  
Why is ReLU often preferred in deep neural networks over Sigmoid or Tanh?  

---



<div class="lesson-nav" style="display:flex; justify-content:space-between; margin-top:2em;">
  <a class="btn btn--inverse" href="{{ '/teaching/ai/hw1' | relative_url }}">⬅︎ Previous: Homework 1 </a>
  <a class="btn btn--primary" href="{{ '/teaching/ai/mp1' | relative_url }}">Next: Mini Project 1 ➡︎</a>
</div>